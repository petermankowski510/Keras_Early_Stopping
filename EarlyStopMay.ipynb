{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mlp overfit on the moons dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=200)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)\n",
    "\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30 samples, validate on 70 samples\n",
      "Epoch 1/4000\n",
      " - 0s - loss: 0.7189 - accuracy: 0.2000 - val_loss: 0.6911 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.84286\n",
      "Epoch 2/4000\n",
      " - 0s - loss: 0.7014 - accuracy: 0.4667 - val_loss: 0.6813 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.84286\n",
      "Epoch 3/4000\n",
      " - 0s - loss: 0.6868 - accuracy: 0.5667 - val_loss: 0.6712 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.84286\n",
      "Epoch 4/4000\n",
      " - 0s - loss: 0.6717 - accuracy: 0.7333 - val_loss: 0.6611 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.84286\n",
      "Epoch 5/4000\n",
      " - 0s - loss: 0.6565 - accuracy: 0.8000 - val_loss: 0.6512 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.84286\n",
      "Epoch 6/4000\n",
      " - 0s - loss: 0.6414 - accuracy: 0.8667 - val_loss: 0.6412 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.84286\n",
      "Epoch 7/4000\n",
      " - 0s - loss: 0.6262 - accuracy: 0.8667 - val_loss: 0.6318 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.84286\n",
      "Epoch 8/4000\n",
      " - 0s - loss: 0.6117 - accuracy: 0.8667 - val_loss: 0.6225 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.84286\n",
      "Epoch 9/4000\n",
      " - 0s - loss: 0.5973 - accuracy: 0.8667 - val_loss: 0.6137 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.84286\n",
      "Epoch 10/4000\n",
      " - 0s - loss: 0.5834 - accuracy: 0.8667 - val_loss: 0.6051 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.84286\n",
      "Epoch 11/4000\n",
      " - 0s - loss: 0.5699 - accuracy: 0.9000 - val_loss: 0.5968 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.84286\n",
      "Epoch 12/4000\n",
      " - 0s - loss: 0.5567 - accuracy: 0.9000 - val_loss: 0.5889 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.84286\n",
      "Epoch 13/4000\n",
      " - 0s - loss: 0.5440 - accuracy: 0.9000 - val_loss: 0.5812 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.84286\n",
      "Epoch 14/4000\n",
      " - 0s - loss: 0.5316 - accuracy: 0.9000 - val_loss: 0.5738 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.84286\n",
      "Epoch 15/4000\n",
      " - 0s - loss: 0.5195 - accuracy: 0.9000 - val_loss: 0.5667 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.84286\n",
      "Epoch 16/4000\n",
      " - 0s - loss: 0.5078 - accuracy: 0.9000 - val_loss: 0.5598 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.84286\n",
      "Epoch 17/4000\n",
      " - 0s - loss: 0.4964 - accuracy: 0.9000 - val_loss: 0.5532 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.84286\n",
      "Epoch 18/4000\n",
      " - 0s - loss: 0.4854 - accuracy: 0.9000 - val_loss: 0.5469 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.84286\n",
      "Epoch 19/4000\n",
      " - 0s - loss: 0.4747 - accuracy: 0.9000 - val_loss: 0.5408 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.84286\n",
      "Epoch 20/4000\n",
      " - 0s - loss: 0.4642 - accuracy: 0.9000 - val_loss: 0.5349 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.84286\n",
      "Epoch 21/4000\n",
      " - 0s - loss: 0.4541 - accuracy: 0.9000 - val_loss: 0.5293 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.84286\n",
      "Epoch 22/4000\n",
      " - 0s - loss: 0.4444 - accuracy: 0.9000 - val_loss: 0.5239 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.84286\n",
      "Epoch 23/4000\n",
      " - 0s - loss: 0.4349 - accuracy: 0.9000 - val_loss: 0.5188 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.84286\n",
      "Epoch 24/4000\n",
      " - 0s - loss: 0.4257 - accuracy: 0.9000 - val_loss: 0.5138 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.84286\n",
      "Epoch 25/4000\n",
      " - 0s - loss: 0.4168 - accuracy: 0.9000 - val_loss: 0.5091 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.84286\n",
      "Epoch 26/4000\n",
      " - 0s - loss: 0.4081 - accuracy: 0.9000 - val_loss: 0.5045 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.84286\n",
      "Epoch 27/4000\n",
      " - 0s - loss: 0.3997 - accuracy: 0.9000 - val_loss: 0.5002 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.84286\n",
      "Epoch 28/4000\n",
      " - 0s - loss: 0.3916 - accuracy: 0.9000 - val_loss: 0.4961 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.84286\n",
      "Epoch 29/4000\n",
      " - 0s - loss: 0.3837 - accuracy: 0.9000 - val_loss: 0.4922 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.84286\n",
      "Epoch 30/4000\n",
      " - 0s - loss: 0.3761 - accuracy: 0.9000 - val_loss: 0.4884 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.84286\n",
      "Epoch 31/4000\n",
      " - 0s - loss: 0.3687 - accuracy: 0.9000 - val_loss: 0.4848 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.84286\n",
      "Epoch 32/4000\n",
      " - 0s - loss: 0.3616 - accuracy: 0.9000 - val_loss: 0.4814 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.84286\n",
      "Epoch 33/4000\n",
      " - 0s - loss: 0.3547 - accuracy: 0.9000 - val_loss: 0.4782 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.84286\n",
      "Epoch 34/4000\n",
      " - 0s - loss: 0.3480 - accuracy: 0.9000 - val_loss: 0.4752 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.84286\n",
      "Epoch 35/4000\n",
      " - 0s - loss: 0.3416 - accuracy: 0.9000 - val_loss: 0.4723 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.84286\n",
      "Epoch 36/4000\n",
      " - 0s - loss: 0.3353 - accuracy: 0.9000 - val_loss: 0.4696 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.84286\n",
      "Epoch 37/4000\n",
      " - 0s - loss: 0.3293 - accuracy: 0.9000 - val_loss: 0.4671 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.84286\n",
      "Epoch 38/4000\n",
      " - 0s - loss: 0.3235 - accuracy: 0.9000 - val_loss: 0.4647 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.84286\n",
      "Epoch 39/4000\n",
      " - 0s - loss: 0.3179 - accuracy: 0.9000 - val_loss: 0.4624 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.84286\n",
      "Epoch 40/4000\n",
      " - 0s - loss: 0.3125 - accuracy: 0.9000 - val_loss: 0.4603 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.84286\n",
      "Epoch 41/4000\n",
      " - 0s - loss: 0.3073 - accuracy: 0.9000 - val_loss: 0.4583 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.84286\n",
      "Epoch 42/4000\n",
      " - 0s - loss: 0.3023 - accuracy: 0.9000 - val_loss: 0.4565 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.84286\n",
      "Epoch 43/4000\n",
      " - 0s - loss: 0.2975 - accuracy: 0.9000 - val_loss: 0.4548 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.84286\n",
      "Epoch 44/4000\n",
      " - 0s - loss: 0.2929 - accuracy: 0.9000 - val_loss: 0.4532 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.84286\n",
      "Epoch 45/4000\n",
      " - 0s - loss: 0.2884 - accuracy: 0.9000 - val_loss: 0.4516 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.84286\n",
      "Epoch 46/4000\n",
      " - 0s - loss: 0.2841 - accuracy: 0.9000 - val_loss: 0.4503 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.84286\n",
      "Epoch 47/4000\n",
      " - 0s - loss: 0.2801 - accuracy: 0.9000 - val_loss: 0.4490 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.84286\n",
      "Epoch 48/4000\n",
      " - 0s - loss: 0.2761 - accuracy: 0.9000 - val_loss: 0.4478 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.84286\n",
      "Epoch 49/4000\n",
      " - 0s - loss: 0.2723 - accuracy: 0.9000 - val_loss: 0.4466 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.84286\n",
      "Epoch 50/4000\n",
      " - 0s - loss: 0.2686 - accuracy: 0.9000 - val_loss: 0.4456 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.84286\n",
      "Epoch 51/4000\n",
      " - 0s - loss: 0.2651 - accuracy: 0.9000 - val_loss: 0.4446 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.84286\n",
      "Epoch 52/4000\n",
      " - 0s - loss: 0.2618 - accuracy: 0.9000 - val_loss: 0.4437 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.84286\n",
      "Epoch 53/4000\n",
      " - 0s - loss: 0.2585 - accuracy: 0.9000 - val_loss: 0.4428 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.84286\n",
      "Epoch 54/4000\n",
      " - 0s - loss: 0.2554 - accuracy: 0.9000 - val_loss: 0.4421 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.84286\n",
      "Epoch 55/4000\n",
      " - 0s - loss: 0.2524 - accuracy: 0.9000 - val_loss: 0.4413 - val_accuracy: 0.7429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.84286\n",
      "Epoch 56/4000\n",
      " - 0s - loss: 0.2495 - accuracy: 0.9000 - val_loss: 0.4406 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.84286\n",
      "Epoch 57/4000\n",
      " - 0s - loss: 0.2468 - accuracy: 0.9000 - val_loss: 0.4399 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.84286\n",
      "Epoch 58/4000\n",
      " - 0s - loss: 0.2441 - accuracy: 0.9000 - val_loss: 0.4393 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.84286\n",
      "Epoch 59/4000\n",
      " - 0s - loss: 0.2415 - accuracy: 0.9000 - val_loss: 0.4387 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.84286\n",
      "Epoch 60/4000\n",
      " - 0s - loss: 0.2391 - accuracy: 0.9000 - val_loss: 0.4381 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.84286\n",
      "Epoch 61/4000\n",
      " - 0s - loss: 0.2367 - accuracy: 0.9000 - val_loss: 0.4376 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.84286\n",
      "Epoch 62/4000\n",
      " - 0s - loss: 0.2344 - accuracy: 0.9000 - val_loss: 0.4370 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.84286\n",
      "Epoch 63/4000\n",
      " - 0s - loss: 0.2322 - accuracy: 0.9000 - val_loss: 0.4365 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.84286\n",
      "Epoch 64/4000\n",
      " - 0s - loss: 0.2301 - accuracy: 0.9000 - val_loss: 0.4360 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.84286\n",
      "Epoch 65/4000\n",
      " - 0s - loss: 0.2280 - accuracy: 0.9000 - val_loss: 0.4355 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.84286\n",
      "Epoch 66/4000\n",
      " - 0s - loss: 0.2261 - accuracy: 0.9000 - val_loss: 0.4350 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.84286\n",
      "Epoch 67/4000\n",
      " - 0s - loss: 0.2242 - accuracy: 0.9000 - val_loss: 0.4346 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.84286\n",
      "Epoch 68/4000\n",
      " - 0s - loss: 0.2224 - accuracy: 0.9000 - val_loss: 0.4341 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.84286\n",
      "Epoch 69/4000\n",
      " - 0s - loss: 0.2206 - accuracy: 0.9000 - val_loss: 0.4336 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.84286\n",
      "Epoch 70/4000\n",
      " - 0s - loss: 0.2189 - accuracy: 0.9000 - val_loss: 0.4332 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.84286\n",
      "Epoch 71/4000\n",
      " - 0s - loss: 0.2173 - accuracy: 0.9000 - val_loss: 0.4327 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.84286\n",
      "Epoch 72/4000\n",
      " - 0s - loss: 0.2158 - accuracy: 0.9000 - val_loss: 0.4322 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.84286\n",
      "Epoch 73/4000\n",
      " - 0s - loss: 0.2142 - accuracy: 0.9000 - val_loss: 0.4317 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.84286\n",
      "Epoch 74/4000\n",
      " - 0s - loss: 0.2128 - accuracy: 0.9000 - val_loss: 0.4313 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.84286\n",
      "Epoch 75/4000\n",
      " - 0s - loss: 0.2114 - accuracy: 0.9000 - val_loss: 0.4308 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.84286\n",
      "Epoch 76/4000\n",
      " - 0s - loss: 0.2100 - accuracy: 0.9000 - val_loss: 0.4303 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.84286\n",
      "Epoch 77/4000\n",
      " - 0s - loss: 0.2087 - accuracy: 0.9000 - val_loss: 0.4298 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.84286\n",
      "Epoch 78/4000\n",
      " - 0s - loss: 0.2074 - accuracy: 0.9000 - val_loss: 0.4294 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.84286\n",
      "Epoch 79/4000\n",
      " - 0s - loss: 0.2062 - accuracy: 0.9000 - val_loss: 0.4289 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.84286\n",
      "Epoch 80/4000\n",
      " - 0s - loss: 0.2049 - accuracy: 0.9000 - val_loss: 0.4284 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.84286\n",
      "Epoch 81/4000\n",
      " - 0s - loss: 0.2038 - accuracy: 0.9000 - val_loss: 0.4279 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.84286\n",
      "Epoch 82/4000\n",
      " - 0s - loss: 0.2026 - accuracy: 0.9000 - val_loss: 0.4274 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.84286\n",
      "Epoch 83/4000\n",
      " - 0s - loss: 0.2015 - accuracy: 0.9000 - val_loss: 0.4269 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.84286\n",
      "Epoch 84/4000\n",
      " - 0s - loss: 0.2005 - accuracy: 0.9000 - val_loss: 0.4264 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.84286\n",
      "Epoch 85/4000\n",
      " - 0s - loss: 0.1994 - accuracy: 0.9000 - val_loss: 0.4259 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.84286\n",
      "Epoch 86/4000\n",
      " - 0s - loss: 0.1984 - accuracy: 0.9000 - val_loss: 0.4253 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.84286\n",
      "Epoch 87/4000\n",
      " - 0s - loss: 0.1974 - accuracy: 0.9000 - val_loss: 0.4248 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.84286\n",
      "Epoch 88/4000\n",
      " - 0s - loss: 0.1965 - accuracy: 0.9000 - val_loss: 0.4243 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.84286\n",
      "Epoch 89/4000\n",
      " - 0s - loss: 0.1955 - accuracy: 0.9000 - val_loss: 0.4237 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.84286\n",
      "Epoch 90/4000\n",
      " - 0s - loss: 0.1946 - accuracy: 0.9000 - val_loss: 0.4231 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.84286\n",
      "Epoch 91/4000\n",
      " - 0s - loss: 0.1937 - accuracy: 0.9000 - val_loss: 0.4226 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.84286\n",
      "Epoch 92/4000\n",
      " - 0s - loss: 0.1929 - accuracy: 0.9000 - val_loss: 0.4220 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.84286\n",
      "Epoch 93/4000\n",
      " - 0s - loss: 0.1920 - accuracy: 0.9000 - val_loss: 0.4214 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.84286\n",
      "Epoch 94/4000\n",
      " - 0s - loss: 0.1912 - accuracy: 0.9000 - val_loss: 0.4208 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.84286\n",
      "Epoch 95/4000\n",
      " - 0s - loss: 0.1903 - accuracy: 0.9000 - val_loss: 0.4202 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.84286\n",
      "Epoch 96/4000\n",
      " - 0s - loss: 0.1895 - accuracy: 0.9000 - val_loss: 0.4196 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.84286\n",
      "Epoch 97/4000\n",
      " - 0s - loss: 0.1888 - accuracy: 0.9000 - val_loss: 0.4190 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.84286\n",
      "Epoch 98/4000\n",
      " - 0s - loss: 0.1880 - accuracy: 0.9000 - val_loss: 0.4184 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.84286\n",
      "Epoch 99/4000\n",
      " - 0s - loss: 0.1873 - accuracy: 0.9000 - val_loss: 0.4178 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.84286\n",
      "Epoch 100/4000\n",
      " - 0s - loss: 0.1865 - accuracy: 0.9000 - val_loss: 0.4172 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.84286\n",
      "Epoch 101/4000\n",
      " - 0s - loss: 0.1858 - accuracy: 0.9000 - val_loss: 0.4166 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.84286\n",
      "Epoch 102/4000\n",
      " - 0s - loss: 0.1851 - accuracy: 0.9000 - val_loss: 0.4160 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.84286\n",
      "Epoch 103/4000\n",
      " - 0s - loss: 0.1844 - accuracy: 0.9000 - val_loss: 0.4153 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.84286\n",
      "Epoch 104/4000\n",
      " - 0s - loss: 0.1837 - accuracy: 0.9000 - val_loss: 0.4147 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.84286\n",
      "Epoch 105/4000\n",
      " - 0s - loss: 0.1830 - accuracy: 0.9000 - val_loss: 0.4141 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.84286\n",
      "Epoch 106/4000\n",
      " - 0s - loss: 0.1824 - accuracy: 0.9000 - val_loss: 0.4135 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.84286\n",
      "Epoch 107/4000\n",
      " - 0s - loss: 0.1817 - accuracy: 0.9000 - val_loss: 0.4129 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.84286\n",
      "Epoch 108/4000\n",
      " - 0s - loss: 0.1811 - accuracy: 0.9000 - val_loss: 0.4123 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.84286\n",
      "Epoch 109/4000\n",
      " - 0s - loss: 0.1804 - accuracy: 0.9000 - val_loss: 0.4117 - val_accuracy: 0.7571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.84286\n",
      "Epoch 110/4000\n",
      " - 0s - loss: 0.1798 - accuracy: 0.9000 - val_loss: 0.4111 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.84286\n",
      "Epoch 111/4000\n",
      " - 0s - loss: 0.1792 - accuracy: 0.9000 - val_loss: 0.4105 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.84286\n",
      "Epoch 112/4000\n",
      " - 0s - loss: 0.1786 - accuracy: 0.9000 - val_loss: 0.4099 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.84286\n",
      "Epoch 113/4000\n",
      " - 0s - loss: 0.1780 - accuracy: 0.9000 - val_loss: 0.4093 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.84286\n",
      "Epoch 114/4000\n",
      " - 0s - loss: 0.1774 - accuracy: 0.9000 - val_loss: 0.4087 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.84286\n",
      "Epoch 115/4000\n",
      " - 0s - loss: 0.1768 - accuracy: 0.9000 - val_loss: 0.4081 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.84286\n",
      "Epoch 116/4000\n",
      " - 0s - loss: 0.1763 - accuracy: 0.9000 - val_loss: 0.4075 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.84286\n",
      "Epoch 117/4000\n",
      " - 0s - loss: 0.1757 - accuracy: 0.9000 - val_loss: 0.4069 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.84286\n",
      "Epoch 118/4000\n",
      " - 0s - loss: 0.1751 - accuracy: 0.9000 - val_loss: 0.4063 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.84286\n",
      "Epoch 119/4000\n",
      " - 0s - loss: 0.1746 - accuracy: 0.9000 - val_loss: 0.4057 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.84286\n",
      "Epoch 120/4000\n",
      " - 0s - loss: 0.1740 - accuracy: 0.9333 - val_loss: 0.4051 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.84286\n",
      "Epoch 121/4000\n",
      " - 0s - loss: 0.1735 - accuracy: 0.9333 - val_loss: 0.4045 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.84286\n",
      "Epoch 122/4000\n",
      " - 0s - loss: 0.1730 - accuracy: 0.9333 - val_loss: 0.4039 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.84286\n",
      "Epoch 123/4000\n",
      " - 0s - loss: 0.1724 - accuracy: 0.9333 - val_loss: 0.4033 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.84286\n",
      "Epoch 124/4000\n",
      " - 0s - loss: 0.1719 - accuracy: 0.9333 - val_loss: 0.4027 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.84286\n",
      "Epoch 125/4000\n",
      " - 0s - loss: 0.1714 - accuracy: 0.9333 - val_loss: 0.4021 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.84286\n",
      "Epoch 126/4000\n",
      " - 0s - loss: 0.1709 - accuracy: 0.9333 - val_loss: 0.4015 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.84286\n",
      "Epoch 127/4000\n",
      " - 0s - loss: 0.1704 - accuracy: 0.9333 - val_loss: 0.4009 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.84286\n",
      "Epoch 128/4000\n",
      " - 0s - loss: 0.1699 - accuracy: 0.9333 - val_loss: 0.4003 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.84286\n",
      "Epoch 129/4000\n",
      " - 0s - loss: 0.1694 - accuracy: 0.9333 - val_loss: 0.3997 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.84286\n",
      "Epoch 130/4000\n",
      " - 0s - loss: 0.1690 - accuracy: 0.9333 - val_loss: 0.3990 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.84286\n",
      "Epoch 131/4000\n",
      " - 0s - loss: 0.1685 - accuracy: 0.9333 - val_loss: 0.3984 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.84286\n",
      "Epoch 132/4000\n",
      " - 0s - loss: 0.1680 - accuracy: 0.9333 - val_loss: 0.3978 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.84286\n",
      "Epoch 133/4000\n",
      " - 0s - loss: 0.1675 - accuracy: 0.9333 - val_loss: 0.3971 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.84286\n",
      "Epoch 134/4000\n",
      " - 0s - loss: 0.1671 - accuracy: 0.9333 - val_loss: 0.3965 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.84286\n",
      "Epoch 135/4000\n",
      " - 0s - loss: 0.1666 - accuracy: 0.9333 - val_loss: 0.3959 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.84286\n",
      "Epoch 136/4000\n",
      " - 0s - loss: 0.1662 - accuracy: 0.9667 - val_loss: 0.3953 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.84286\n",
      "Epoch 137/4000\n",
      " - 0s - loss: 0.1657 - accuracy: 0.9667 - val_loss: 0.3947 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.84286\n",
      "Epoch 138/4000\n",
      " - 0s - loss: 0.1653 - accuracy: 0.9667 - val_loss: 0.3941 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.84286\n",
      "Epoch 139/4000\n",
      " - 0s - loss: 0.1648 - accuracy: 0.9667 - val_loss: 0.3934 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.84286\n",
      "Epoch 140/4000\n",
      " - 0s - loss: 0.1644 - accuracy: 0.9667 - val_loss: 0.3928 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.84286\n",
      "Epoch 141/4000\n",
      " - 0s - loss: 0.1639 - accuracy: 0.9667 - val_loss: 0.3922 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.84286\n",
      "Epoch 142/4000\n",
      " - 0s - loss: 0.1635 - accuracy: 0.9667 - val_loss: 0.3915 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.84286\n",
      "Epoch 143/4000\n",
      " - 0s - loss: 0.1631 - accuracy: 0.9667 - val_loss: 0.3909 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.84286\n",
      "Epoch 144/4000\n",
      " - 0s - loss: 0.1627 - accuracy: 0.9667 - val_loss: 0.3903 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.84286\n",
      "Epoch 145/4000\n",
      " - 0s - loss: 0.1623 - accuracy: 0.9667 - val_loss: 0.3897 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.84286\n",
      "Epoch 146/4000\n",
      " - 0s - loss: 0.1619 - accuracy: 0.9667 - val_loss: 0.3892 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.84286\n",
      "Epoch 147/4000\n",
      " - 0s - loss: 0.1614 - accuracy: 0.9667 - val_loss: 0.3886 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.84286\n",
      "Epoch 148/4000\n",
      " - 0s - loss: 0.1610 - accuracy: 0.9667 - val_loss: 0.3880 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.84286\n",
      "Epoch 149/4000\n",
      " - 0s - loss: 0.1606 - accuracy: 0.9667 - val_loss: 0.3874 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.84286\n",
      "Epoch 150/4000\n",
      " - 0s - loss: 0.1602 - accuracy: 0.9667 - val_loss: 0.3868 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.84286\n",
      "Epoch 151/4000\n",
      " - 0s - loss: 0.1599 - accuracy: 0.9667 - val_loss: 0.3862 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.84286\n",
      "Epoch 152/4000\n",
      " - 0s - loss: 0.1595 - accuracy: 0.9667 - val_loss: 0.3857 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.84286\n",
      "Epoch 153/4000\n",
      " - 0s - loss: 0.1591 - accuracy: 0.9667 - val_loss: 0.3851 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.84286\n",
      "Epoch 154/4000\n",
      " - 0s - loss: 0.1587 - accuracy: 0.9667 - val_loss: 0.3846 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.84286\n",
      "Epoch 155/4000\n",
      " - 0s - loss: 0.1584 - accuracy: 0.9667 - val_loss: 0.3840 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.84286\n",
      "Epoch 156/4000\n",
      " - 0s - loss: 0.1580 - accuracy: 0.9667 - val_loss: 0.3835 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.84286\n",
      "Epoch 157/4000\n",
      " - 0s - loss: 0.1576 - accuracy: 0.9667 - val_loss: 0.3830 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.84286\n",
      "Epoch 158/4000\n",
      " - 0s - loss: 0.1573 - accuracy: 0.9667 - val_loss: 0.3825 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.84286\n",
      "Epoch 159/4000\n",
      " - 0s - loss: 0.1569 - accuracy: 0.9667 - val_loss: 0.3820 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.84286\n",
      "Epoch 160/4000\n",
      " - 0s - loss: 0.1566 - accuracy: 0.9667 - val_loss: 0.3815 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.84286\n",
      "Epoch 161/4000\n",
      " - 0s - loss: 0.1562 - accuracy: 0.9667 - val_loss: 0.3810 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.84286\n",
      "Epoch 162/4000\n",
      " - 0s - loss: 0.1559 - accuracy: 0.9667 - val_loss: 0.3806 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.84286\n",
      "Epoch 163/4000\n",
      " - 0s - loss: 0.1556 - accuracy: 0.9667 - val_loss: 0.3801 - val_accuracy: 0.8286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.84286\n",
      "Epoch 164/4000\n",
      " - 0s - loss: 0.1552 - accuracy: 0.9667 - val_loss: 0.3797 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.84286\n",
      "Epoch 165/4000\n",
      " - 0s - loss: 0.1549 - accuracy: 0.9667 - val_loss: 0.3792 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.84286\n",
      "Epoch 166/4000\n",
      " - 0s - loss: 0.1546 - accuracy: 0.9667 - val_loss: 0.3788 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.84286\n",
      "Epoch 167/4000\n",
      " - 0s - loss: 0.1543 - accuracy: 0.9667 - val_loss: 0.3784 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.84286\n",
      "Epoch 168/4000\n",
      " - 0s - loss: 0.1539 - accuracy: 0.9667 - val_loss: 0.3780 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.84286\n",
      "Epoch 169/4000\n",
      " - 0s - loss: 0.1536 - accuracy: 0.9667 - val_loss: 0.3777 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.84286\n",
      "Epoch 170/4000\n",
      " - 0s - loss: 0.1533 - accuracy: 0.9667 - val_loss: 0.3773 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.84286\n",
      "Epoch 171/4000\n",
      " - 0s - loss: 0.1530 - accuracy: 0.9667 - val_loss: 0.3769 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.84286\n",
      "Epoch 172/4000\n",
      " - 0s - loss: 0.1527 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.84286\n",
      "Epoch 173/4000\n",
      " - 0s - loss: 0.1524 - accuracy: 0.9667 - val_loss: 0.3761 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.84286\n",
      "Epoch 174/4000\n",
      " - 0s - loss: 0.1521 - accuracy: 0.9667 - val_loss: 0.3758 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.84286\n",
      "Epoch 175/4000\n",
      " - 0s - loss: 0.1519 - accuracy: 0.9667 - val_loss: 0.3754 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.84286\n",
      "Epoch 176/4000\n",
      " - 0s - loss: 0.1516 - accuracy: 0.9667 - val_loss: 0.3750 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.84286\n",
      "Epoch 177/4000\n",
      " - 0s - loss: 0.1513 - accuracy: 0.9667 - val_loss: 0.3747 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.84286\n",
      "Epoch 178/4000\n",
      " - 0s - loss: 0.1510 - accuracy: 0.9667 - val_loss: 0.3744 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.84286\n",
      "Epoch 179/4000\n",
      " - 0s - loss: 0.1507 - accuracy: 0.9667 - val_loss: 0.3740 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.84286\n",
      "Epoch 180/4000\n",
      " - 0s - loss: 0.1505 - accuracy: 0.9667 - val_loss: 0.3737 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.84286\n",
      "Epoch 181/4000\n",
      " - 0s - loss: 0.1502 - accuracy: 0.9667 - val_loss: 0.3735 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.84286\n",
      "Epoch 182/4000\n",
      " - 0s - loss: 0.1500 - accuracy: 0.9667 - val_loss: 0.3732 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.84286\n",
      "Epoch 183/4000\n",
      " - 0s - loss: 0.1497 - accuracy: 0.9667 - val_loss: 0.3729 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.84286\n",
      "Epoch 184/4000\n",
      " - 0s - loss: 0.1495 - accuracy: 0.9667 - val_loss: 0.3727 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.84286\n",
      "Epoch 185/4000\n",
      " - 0s - loss: 0.1492 - accuracy: 0.9667 - val_loss: 0.3725 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.84286\n",
      "Epoch 186/4000\n",
      " - 0s - loss: 0.1490 - accuracy: 0.9667 - val_loss: 0.3722 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.84286\n",
      "Epoch 187/4000\n",
      " - 0s - loss: 0.1487 - accuracy: 0.9667 - val_loss: 0.3720 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.84286\n",
      "Epoch 188/4000\n",
      " - 0s - loss: 0.1485 - accuracy: 0.9667 - val_loss: 0.3718 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.84286\n",
      "Epoch 189/4000\n",
      " - 0s - loss: 0.1483 - accuracy: 0.9667 - val_loss: 0.3716 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.84286\n",
      "Epoch 190/4000\n",
      " - 0s - loss: 0.1481 - accuracy: 0.9667 - val_loss: 0.3714 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.84286\n",
      "Epoch 191/4000\n",
      " - 0s - loss: 0.1478 - accuracy: 0.9667 - val_loss: 0.3712 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.84286\n",
      "Epoch 192/4000\n",
      " - 0s - loss: 0.1476 - accuracy: 0.9667 - val_loss: 0.3710 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.84286\n",
      "Epoch 193/4000\n",
      " - 0s - loss: 0.1474 - accuracy: 0.9667 - val_loss: 0.3708 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.84286\n",
      "Epoch 194/4000\n",
      " - 0s - loss: 0.1472 - accuracy: 0.9667 - val_loss: 0.3706 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.84286\n",
      "Epoch 195/4000\n",
      " - 0s - loss: 0.1470 - accuracy: 0.9667 - val_loss: 0.3705 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.84286\n",
      "Epoch 196/4000\n",
      " - 0s - loss: 0.1468 - accuracy: 0.9667 - val_loss: 0.3704 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.84286\n",
      "Epoch 197/4000\n",
      " - 0s - loss: 0.1466 - accuracy: 0.9667 - val_loss: 0.3702 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.84286\n",
      "Epoch 198/4000\n",
      " - 0s - loss: 0.1464 - accuracy: 0.9667 - val_loss: 0.3701 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.84286\n",
      "Epoch 199/4000\n",
      " - 0s - loss: 0.1462 - accuracy: 0.9667 - val_loss: 0.3700 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.84286\n",
      "Epoch 200/4000\n",
      " - 0s - loss: 0.1460 - accuracy: 0.9667 - val_loss: 0.3699 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.84286\n",
      "Epoch 201/4000\n",
      " - 0s - loss: 0.1458 - accuracy: 0.9667 - val_loss: 0.3698 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00201: val_accuracy did not improve from 0.84286\n",
      "Epoch 202/4000\n",
      " - 0s - loss: 0.1456 - accuracy: 0.9667 - val_loss: 0.3697 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00202: val_accuracy did not improve from 0.84286\n",
      "Epoch 203/4000\n",
      " - 0s - loss: 0.1454 - accuracy: 0.9667 - val_loss: 0.3696 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00203: val_accuracy did not improve from 0.84286\n",
      "Epoch 204/4000\n",
      " - 0s - loss: 0.1452 - accuracy: 0.9667 - val_loss: 0.3695 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00204: val_accuracy did not improve from 0.84286\n",
      "Epoch 205/4000\n",
      " - 0s - loss: 0.1451 - accuracy: 0.9667 - val_loss: 0.3694 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00205: val_accuracy did not improve from 0.84286\n",
      "Epoch 206/4000\n",
      " - 0s - loss: 0.1449 - accuracy: 0.9667 - val_loss: 0.3693 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00206: val_accuracy did not improve from 0.84286\n",
      "Epoch 207/4000\n",
      " - 0s - loss: 0.1447 - accuracy: 0.9667 - val_loss: 0.3693 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00207: val_accuracy did not improve from 0.84286\n",
      "Epoch 208/4000\n",
      " - 0s - loss: 0.1446 - accuracy: 0.9667 - val_loss: 0.3692 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00208: val_accuracy did not improve from 0.84286\n",
      "Epoch 209/4000\n",
      " - 0s - loss: 0.1444 - accuracy: 0.9667 - val_loss: 0.3692 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00209: val_accuracy did not improve from 0.84286\n",
      "Epoch 210/4000\n",
      " - 0s - loss: 0.1442 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00210: val_accuracy did not improve from 0.84286\n",
      "Epoch 211/4000\n",
      " - 0s - loss: 0.1441 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00211: val_accuracy did not improve from 0.84286\n",
      "Epoch 212/4000\n",
      " - 0s - loss: 0.1439 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00212: val_accuracy did not improve from 0.84286\n",
      "Epoch 213/4000\n",
      " - 0s - loss: 0.1438 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00213: val_accuracy did not improve from 0.84286\n",
      "Epoch 214/4000\n",
      " - 0s - loss: 0.1436 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00214: val_accuracy did not improve from 0.84286\n",
      "Epoch 215/4000\n",
      " - 0s - loss: 0.1435 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00215: val_accuracy did not improve from 0.84286\n",
      "Epoch 216/4000\n",
      " - 0s - loss: 0.1433 - accuracy: 0.9667 - val_loss: 0.3690 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00216: val_accuracy did not improve from 0.84286\n",
      "Epoch 217/4000\n",
      " - 0s - loss: 0.1432 - accuracy: 0.9667 - val_loss: 0.3690 - val_accuracy: 0.8143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00217: val_accuracy did not improve from 0.84286\n",
      "Epoch 218/4000\n",
      " - 0s - loss: 0.1430 - accuracy: 0.9667 - val_loss: 0.3690 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00218: val_accuracy did not improve from 0.84286\n",
      "Epoch 219/4000\n",
      " - 0s - loss: 0.1429 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00219: val_accuracy did not improve from 0.84286\n",
      "Epoch 220/4000\n",
      " - 0s - loss: 0.1428 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00220: val_accuracy did not improve from 0.84286\n",
      "Epoch 221/4000\n",
      " - 0s - loss: 0.1426 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00221: val_accuracy did not improve from 0.84286\n",
      "Epoch 222/4000\n",
      " - 0s - loss: 0.1425 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00222: val_accuracy did not improve from 0.84286\n",
      "Epoch 223/4000\n",
      " - 0s - loss: 0.1424 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00223: val_accuracy did not improve from 0.84286\n",
      "Epoch 224/4000\n",
      " - 0s - loss: 0.1422 - accuracy: 0.9667 - val_loss: 0.3692 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00224: val_accuracy did not improve from 0.84286\n",
      "Epoch 225/4000\n",
      " - 0s - loss: 0.1421 - accuracy: 0.9667 - val_loss: 0.3692 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00225: val_accuracy did not improve from 0.84286\n",
      "Epoch 226/4000\n",
      " - 0s - loss: 0.1420 - accuracy: 0.9667 - val_loss: 0.3692 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00226: val_accuracy did not improve from 0.84286\n",
      "Epoch 227/4000\n",
      " - 0s - loss: 0.1419 - accuracy: 0.9667 - val_loss: 0.3693 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00227: val_accuracy did not improve from 0.84286\n",
      "Epoch 228/4000\n",
      " - 0s - loss: 0.1418 - accuracy: 0.9667 - val_loss: 0.3693 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00228: val_accuracy did not improve from 0.84286\n",
      "Epoch 229/4000\n",
      " - 0s - loss: 0.1416 - accuracy: 0.9667 - val_loss: 0.3693 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00229: val_accuracy did not improve from 0.84286\n",
      "Epoch 230/4000\n",
      " - 0s - loss: 0.1415 - accuracy: 0.9667 - val_loss: 0.3694 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00230: val_accuracy did not improve from 0.84286\n",
      "Epoch 231/4000\n",
      " - 0s - loss: 0.1414 - accuracy: 0.9667 - val_loss: 0.3694 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00231: val_accuracy did not improve from 0.84286\n",
      "Epoch 232/4000\n",
      " - 0s - loss: 0.1413 - accuracy: 0.9667 - val_loss: 0.3695 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00232: val_accuracy did not improve from 0.84286\n",
      "Epoch 233/4000\n",
      " - 0s - loss: 0.1412 - accuracy: 0.9667 - val_loss: 0.3696 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00233: val_accuracy did not improve from 0.84286\n",
      "Epoch 234/4000\n",
      " - 0s - loss: 0.1411 - accuracy: 0.9667 - val_loss: 0.3696 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00234: val_accuracy did not improve from 0.84286\n",
      "Epoch 235/4000\n",
      " - 0s - loss: 0.1410 - accuracy: 0.9667 - val_loss: 0.3697 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00235: val_accuracy did not improve from 0.84286\n",
      "Epoch 236/4000\n",
      " - 0s - loss: 0.1409 - accuracy: 0.9667 - val_loss: 0.3698 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00236: val_accuracy did not improve from 0.84286\n",
      "Epoch 237/4000\n",
      " - 0s - loss: 0.1408 - accuracy: 0.9667 - val_loss: 0.3699 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00237: val_accuracy did not improve from 0.84286\n",
      "Epoch 238/4000\n",
      " - 0s - loss: 0.1407 - accuracy: 0.9667 - val_loss: 0.3699 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00238: val_accuracy did not improve from 0.84286\n",
      "Epoch 239/4000\n",
      " - 0s - loss: 0.1406 - accuracy: 0.9667 - val_loss: 0.3700 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00239: val_accuracy did not improve from 0.84286\n",
      "Epoch 240/4000\n",
      " - 0s - loss: 0.1405 - accuracy: 0.9667 - val_loss: 0.3700 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00240: val_accuracy did not improve from 0.84286\n",
      "Epoch 241/4000\n",
      " - 0s - loss: 0.1404 - accuracy: 0.9667 - val_loss: 0.3701 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00241: val_accuracy did not improve from 0.84286\n",
      "Epoch 242/4000\n",
      " - 0s - loss: 0.1403 - accuracy: 0.9667 - val_loss: 0.3702 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00242: val_accuracy did not improve from 0.84286\n",
      "Epoch 243/4000\n",
      " - 0s - loss: 0.1402 - accuracy: 0.9667 - val_loss: 0.3703 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00243: val_accuracy did not improve from 0.84286\n",
      "Epoch 244/4000\n",
      " - 0s - loss: 0.1401 - accuracy: 0.9667 - val_loss: 0.3704 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00244: val_accuracy did not improve from 0.84286\n",
      "Epoch 245/4000\n",
      " - 0s - loss: 0.1400 - accuracy: 0.9667 - val_loss: 0.3705 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00245: val_accuracy did not improve from 0.84286\n",
      "Epoch 246/4000\n",
      " - 0s - loss: 0.1399 - accuracy: 0.9667 - val_loss: 0.3705 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00246: val_accuracy did not improve from 0.84286\n",
      "Epoch 247/4000\n",
      " - 0s - loss: 0.1398 - accuracy: 0.9667 - val_loss: 0.3706 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00247: val_accuracy did not improve from 0.84286\n",
      "Epoch 248/4000\n",
      " - 0s - loss: 0.1397 - accuracy: 0.9667 - val_loss: 0.3707 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00248: val_accuracy did not improve from 0.84286\n",
      "Epoch 249/4000\n",
      " - 0s - loss: 0.1396 - accuracy: 0.9667 - val_loss: 0.3708 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00249: val_accuracy did not improve from 0.84286\n",
      "Epoch 250/4000\n",
      " - 0s - loss: 0.1396 - accuracy: 0.9667 - val_loss: 0.3709 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00250: val_accuracy did not improve from 0.84286\n",
      "Epoch 251/4000\n",
      " - 0s - loss: 0.1395 - accuracy: 0.9667 - val_loss: 0.3710 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00251: val_accuracy did not improve from 0.84286\n",
      "Epoch 252/4000\n",
      " - 0s - loss: 0.1394 - accuracy: 0.9667 - val_loss: 0.3711 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00252: val_accuracy did not improve from 0.84286\n",
      "Epoch 253/4000\n",
      " - 0s - loss: 0.1393 - accuracy: 0.9667 - val_loss: 0.3712 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00253: val_accuracy did not improve from 0.84286\n",
      "Epoch 254/4000\n",
      " - 0s - loss: 0.1392 - accuracy: 0.9667 - val_loss: 0.3713 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00254: val_accuracy did not improve from 0.84286\n",
      "Epoch 255/4000\n",
      " - 0s - loss: 0.1391 - accuracy: 0.9667 - val_loss: 0.3713 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00255: val_accuracy did not improve from 0.84286\n",
      "Epoch 256/4000\n",
      " - 0s - loss: 0.1391 - accuracy: 0.9667 - val_loss: 0.3714 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00256: val_accuracy did not improve from 0.84286\n",
      "Epoch 257/4000\n",
      " - 0s - loss: 0.1390 - accuracy: 0.9667 - val_loss: 0.3715 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00257: val_accuracy did not improve from 0.84286\n",
      "Epoch 258/4000\n",
      " - 0s - loss: 0.1389 - accuracy: 0.9667 - val_loss: 0.3716 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00258: val_accuracy did not improve from 0.84286\n",
      "Epoch 259/4000\n",
      " - 0s - loss: 0.1388 - accuracy: 0.9667 - val_loss: 0.3717 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00259: val_accuracy did not improve from 0.84286\n",
      "Epoch 260/4000\n",
      " - 0s - loss: 0.1387 - accuracy: 0.9667 - val_loss: 0.3718 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00260: val_accuracy did not improve from 0.84286\n",
      "Epoch 261/4000\n",
      " - 0s - loss: 0.1387 - accuracy: 0.9667 - val_loss: 0.3719 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00261: val_accuracy did not improve from 0.84286\n",
      "Epoch 262/4000\n",
      " - 0s - loss: 0.1386 - accuracy: 0.9667 - val_loss: 0.3720 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00262: val_accuracy did not improve from 0.84286\n",
      "Epoch 263/4000\n",
      " - 0s - loss: 0.1385 - accuracy: 0.9667 - val_loss: 0.3721 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00263: val_accuracy did not improve from 0.84286\n",
      "Epoch 264/4000\n",
      " - 0s - loss: 0.1384 - accuracy: 0.9667 - val_loss: 0.3721 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00264: val_accuracy did not improve from 0.84286\n",
      "Epoch 265/4000\n",
      " - 0s - loss: 0.1384 - accuracy: 0.9667 - val_loss: 0.3722 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00265: val_accuracy did not improve from 0.84286\n",
      "Epoch 266/4000\n",
      " - 0s - loss: 0.1383 - accuracy: 0.9667 - val_loss: 0.3723 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00266: val_accuracy did not improve from 0.84286\n",
      "Epoch 267/4000\n",
      " - 0s - loss: 0.1382 - accuracy: 0.9667 - val_loss: 0.3724 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00267: val_accuracy did not improve from 0.84286\n",
      "Epoch 268/4000\n",
      " - 0s - loss: 0.1381 - accuracy: 0.9667 - val_loss: 0.3725 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00268: val_accuracy did not improve from 0.84286\n",
      "Epoch 269/4000\n",
      " - 0s - loss: 0.1381 - accuracy: 0.9667 - val_loss: 0.3726 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00269: val_accuracy did not improve from 0.84286\n",
      "Epoch 270/4000\n",
      " - 0s - loss: 0.1380 - accuracy: 0.9667 - val_loss: 0.3727 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00270: val_accuracy did not improve from 0.84286\n",
      "Epoch 271/4000\n",
      " - 0s - loss: 0.1379 - accuracy: 0.9667 - val_loss: 0.3727 - val_accuracy: 0.8143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00271: val_accuracy did not improve from 0.84286\n",
      "Epoch 272/4000\n",
      " - 0s - loss: 0.1379 - accuracy: 0.9667 - val_loss: 0.3728 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00272: val_accuracy did not improve from 0.84286\n",
      "Epoch 273/4000\n",
      " - 0s - loss: 0.1378 - accuracy: 0.9667 - val_loss: 0.3729 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00273: val_accuracy did not improve from 0.84286\n",
      "Epoch 274/4000\n",
      " - 0s - loss: 0.1377 - accuracy: 0.9667 - val_loss: 0.3729 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00274: val_accuracy did not improve from 0.84286\n",
      "Epoch 275/4000\n",
      " - 0s - loss: 0.1377 - accuracy: 0.9667 - val_loss: 0.3729 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00275: val_accuracy did not improve from 0.84286\n",
      "Epoch 276/4000\n",
      " - 0s - loss: 0.1376 - accuracy: 0.9667 - val_loss: 0.3730 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00276: val_accuracy did not improve from 0.84286\n",
      "Epoch 277/4000\n",
      " - 0s - loss: 0.1375 - accuracy: 0.9667 - val_loss: 0.3730 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00277: val_accuracy did not improve from 0.84286\n",
      "Epoch 278/4000\n",
      " - 0s - loss: 0.1375 - accuracy: 0.9667 - val_loss: 0.3730 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00278: val_accuracy did not improve from 0.84286\n",
      "Epoch 279/4000\n",
      " - 0s - loss: 0.1374 - accuracy: 0.9667 - val_loss: 0.3731 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00279: val_accuracy did not improve from 0.84286\n",
      "Epoch 280/4000\n",
      " - 0s - loss: 0.1373 - accuracy: 0.9667 - val_loss: 0.3732 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00280: val_accuracy did not improve from 0.84286\n",
      "Epoch 281/4000\n",
      " - 0s - loss: 0.1373 - accuracy: 0.9667 - val_loss: 0.3733 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00281: val_accuracy did not improve from 0.84286\n",
      "Epoch 282/4000\n",
      " - 0s - loss: 0.1372 - accuracy: 0.9667 - val_loss: 0.3734 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00282: val_accuracy did not improve from 0.84286\n",
      "Epoch 283/4000\n",
      " - 0s - loss: 0.1372 - accuracy: 0.9667 - val_loss: 0.3735 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00283: val_accuracy did not improve from 0.84286\n",
      "Epoch 284/4000\n",
      " - 0s - loss: 0.1371 - accuracy: 0.9667 - val_loss: 0.3736 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00284: val_accuracy did not improve from 0.84286\n",
      "Epoch 285/4000\n",
      " - 0s - loss: 0.1370 - accuracy: 0.9667 - val_loss: 0.3737 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00285: val_accuracy did not improve from 0.84286\n",
      "Epoch 286/4000\n",
      " - 0s - loss: 0.1370 - accuracy: 0.9667 - val_loss: 0.3737 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00286: val_accuracy did not improve from 0.84286\n",
      "Epoch 287/4000\n",
      " - 0s - loss: 0.1369 - accuracy: 0.9667 - val_loss: 0.3738 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00287: val_accuracy did not improve from 0.84286\n",
      "Epoch 288/4000\n",
      " - 0s - loss: 0.1368 - accuracy: 0.9667 - val_loss: 0.3739 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00288: val_accuracy did not improve from 0.84286\n",
      "Epoch 289/4000\n",
      " - 0s - loss: 0.1368 - accuracy: 0.9667 - val_loss: 0.3739 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00289: val_accuracy did not improve from 0.84286\n",
      "Epoch 290/4000\n",
      " - 0s - loss: 0.1367 - accuracy: 0.9667 - val_loss: 0.3740 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00290: val_accuracy did not improve from 0.84286\n",
      "Epoch 291/4000\n",
      " - 0s - loss: 0.1367 - accuracy: 0.9667 - val_loss: 0.3741 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00291: val_accuracy did not improve from 0.84286\n",
      "Epoch 292/4000\n",
      " - 0s - loss: 0.1366 - accuracy: 0.9667 - val_loss: 0.3742 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00292: val_accuracy did not improve from 0.84286\n",
      "Epoch 293/4000\n",
      " - 0s - loss: 0.1365 - accuracy: 0.9667 - val_loss: 0.3743 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00293: val_accuracy did not improve from 0.84286\n",
      "Epoch 294/4000\n",
      " - 0s - loss: 0.1365 - accuracy: 0.9667 - val_loss: 0.3743 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00294: val_accuracy did not improve from 0.84286\n",
      "Epoch 295/4000\n",
      " - 0s - loss: 0.1364 - accuracy: 0.9667 - val_loss: 0.3744 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00295: val_accuracy did not improve from 0.84286\n",
      "Epoch 296/4000\n",
      " - 0s - loss: 0.1364 - accuracy: 0.9667 - val_loss: 0.3745 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00296: val_accuracy did not improve from 0.84286\n",
      "Epoch 297/4000\n",
      " - 0s - loss: 0.1363 - accuracy: 0.9667 - val_loss: 0.3746 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00297: val_accuracy did not improve from 0.84286\n",
      "Epoch 298/4000\n",
      " - 0s - loss: 0.1363 - accuracy: 0.9667 - val_loss: 0.3747 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00298: val_accuracy did not improve from 0.84286\n",
      "Epoch 299/4000\n",
      " - 0s - loss: 0.1362 - accuracy: 0.9667 - val_loss: 0.3748 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00299: val_accuracy did not improve from 0.84286\n",
      "Epoch 300/4000\n",
      " - 0s - loss: 0.1361 - accuracy: 0.9667 - val_loss: 0.3748 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00300: val_accuracy did not improve from 0.84286\n",
      "Epoch 301/4000\n",
      " - 0s - loss: 0.1361 - accuracy: 0.9667 - val_loss: 0.3749 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00301: val_accuracy did not improve from 0.84286\n",
      "Epoch 302/4000\n",
      " - 0s - loss: 0.1360 - accuracy: 0.9667 - val_loss: 0.3749 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00302: val_accuracy did not improve from 0.84286\n",
      "Epoch 303/4000\n",
      " - 0s - loss: 0.1360 - accuracy: 0.9667 - val_loss: 0.3750 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00303: val_accuracy did not improve from 0.84286\n",
      "Epoch 304/4000\n",
      " - 0s - loss: 0.1359 - accuracy: 0.9667 - val_loss: 0.3751 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00304: val_accuracy did not improve from 0.84286\n",
      "Epoch 305/4000\n",
      " - 0s - loss: 0.1358 - accuracy: 0.9667 - val_loss: 0.3751 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00305: val_accuracy did not improve from 0.84286\n",
      "Epoch 306/4000\n",
      " - 0s - loss: 0.1358 - accuracy: 0.9667 - val_loss: 0.3752 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00306: val_accuracy did not improve from 0.84286\n",
      "Epoch 307/4000\n",
      " - 0s - loss: 0.1357 - accuracy: 0.9667 - val_loss: 0.3752 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00307: val_accuracy did not improve from 0.84286\n",
      "Epoch 308/4000\n",
      " - 0s - loss: 0.1357 - accuracy: 0.9667 - val_loss: 0.3753 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00308: val_accuracy did not improve from 0.84286\n",
      "Epoch 309/4000\n",
      " - 0s - loss: 0.1356 - accuracy: 0.9667 - val_loss: 0.3753 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00309: val_accuracy did not improve from 0.84286\n",
      "Epoch 310/4000\n",
      " - 0s - loss: 0.1356 - accuracy: 0.9667 - val_loss: 0.3754 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00310: val_accuracy did not improve from 0.84286\n",
      "Epoch 311/4000\n",
      " - 0s - loss: 0.1355 - accuracy: 0.9667 - val_loss: 0.3755 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00311: val_accuracy did not improve from 0.84286\n",
      "Epoch 312/4000\n",
      " - 0s - loss: 0.1354 - accuracy: 0.9667 - val_loss: 0.3755 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00312: val_accuracy did not improve from 0.84286\n",
      "Epoch 313/4000\n",
      " - 0s - loss: 0.1354 - accuracy: 0.9667 - val_loss: 0.3756 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00313: val_accuracy did not improve from 0.84286\n",
      "Epoch 314/4000\n",
      " - 0s - loss: 0.1353 - accuracy: 0.9667 - val_loss: 0.3756 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00314: val_accuracy did not improve from 0.84286\n",
      "Epoch 315/4000\n",
      " - 0s - loss: 0.1353 - accuracy: 0.9667 - val_loss: 0.3757 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00315: val_accuracy did not improve from 0.84286\n",
      "Epoch 316/4000\n",
      " - 0s - loss: 0.1352 - accuracy: 0.9667 - val_loss: 0.3757 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00316: val_accuracy did not improve from 0.84286\n",
      "Epoch 317/4000\n",
      " - 0s - loss: 0.1352 - accuracy: 0.9667 - val_loss: 0.3758 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00317: val_accuracy did not improve from 0.84286\n",
      "Epoch 318/4000\n",
      " - 0s - loss: 0.1351 - accuracy: 0.9667 - val_loss: 0.3758 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00318: val_accuracy did not improve from 0.84286\n",
      "Epoch 319/4000\n",
      " - 0s - loss: 0.1350 - accuracy: 0.9667 - val_loss: 0.3758 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00319: val_accuracy did not improve from 0.84286\n",
      "Epoch 320/4000\n",
      " - 0s - loss: 0.1350 - accuracy: 0.9667 - val_loss: 0.3759 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00320: val_accuracy did not improve from 0.84286\n",
      "Epoch 321/4000\n",
      " - 0s - loss: 0.1349 - accuracy: 0.9667 - val_loss: 0.3759 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00321: val_accuracy did not improve from 0.84286\n",
      "Epoch 322/4000\n",
      " - 0s - loss: 0.1349 - accuracy: 0.9667 - val_loss: 0.3759 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00322: val_accuracy did not improve from 0.84286\n",
      "Epoch 323/4000\n",
      " - 0s - loss: 0.1348 - accuracy: 0.9667 - val_loss: 0.3759 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00323: val_accuracy did not improve from 0.84286\n",
      "Epoch 324/4000\n",
      " - 0s - loss: 0.1347 - accuracy: 0.9667 - val_loss: 0.3760 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00324: val_accuracy did not improve from 0.84286\n",
      "Epoch 325/4000\n",
      " - 0s - loss: 0.1347 - accuracy: 0.9667 - val_loss: 0.3760 - val_accuracy: 0.8429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00325: val_accuracy did not improve from 0.84286\n",
      "Epoch 326/4000\n",
      " - 0s - loss: 0.1346 - accuracy: 0.9667 - val_loss: 0.3760 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00326: val_accuracy did not improve from 0.84286\n",
      "Epoch 327/4000\n",
      " - 0s - loss: 0.1346 - accuracy: 0.9667 - val_loss: 0.3761 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00327: val_accuracy did not improve from 0.84286\n",
      "Epoch 328/4000\n",
      " - 0s - loss: 0.1345 - accuracy: 0.9667 - val_loss: 0.3761 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00328: val_accuracy did not improve from 0.84286\n",
      "Epoch 329/4000\n",
      " - 0s - loss: 0.1344 - accuracy: 0.9667 - val_loss: 0.3762 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00329: val_accuracy did not improve from 0.84286\n",
      "Epoch 330/4000\n",
      " - 0s - loss: 0.1344 - accuracy: 0.9667 - val_loss: 0.3762 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00330: val_accuracy did not improve from 0.84286\n",
      "Epoch 331/4000\n",
      " - 0s - loss: 0.1343 - accuracy: 0.9667 - val_loss: 0.3763 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00331: val_accuracy did not improve from 0.84286\n",
      "Epoch 332/4000\n",
      " - 0s - loss: 0.1343 - accuracy: 0.9667 - val_loss: 0.3763 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00332: val_accuracy did not improve from 0.84286\n",
      "Epoch 333/4000\n",
      " - 0s - loss: 0.1342 - accuracy: 0.9667 - val_loss: 0.3763 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00333: val_accuracy did not improve from 0.84286\n",
      "Epoch 334/4000\n",
      " - 0s - loss: 0.1341 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00334: val_accuracy did not improve from 0.84286\n",
      "Epoch 335/4000\n",
      " - 0s - loss: 0.1341 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00335: val_accuracy did not improve from 0.84286\n",
      "Epoch 336/4000\n",
      " - 0s - loss: 0.1340 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00336: val_accuracy did not improve from 0.84286\n",
      "Epoch 337/4000\n",
      " - 0s - loss: 0.1340 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00337: val_accuracy did not improve from 0.84286\n",
      "Epoch 338/4000\n",
      " - 0s - loss: 0.1339 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00338: val_accuracy did not improve from 0.84286\n",
      "Epoch 339/4000\n",
      " - 0s - loss: 0.1338 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00339: val_accuracy did not improve from 0.84286\n",
      "Epoch 340/4000\n",
      " - 0s - loss: 0.1338 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00340: val_accuracy did not improve from 0.84286\n",
      "Epoch 341/4000\n",
      " - 0s - loss: 0.1337 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00341: val_accuracy did not improve from 0.84286\n",
      "Epoch 342/4000\n",
      " - 0s - loss: 0.1336 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00342: val_accuracy did not improve from 0.84286\n",
      "Epoch 343/4000\n",
      " - 0s - loss: 0.1336 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00343: val_accuracy did not improve from 0.84286\n",
      "Epoch 344/4000\n",
      " - 0s - loss: 0.1335 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00344: val_accuracy did not improve from 0.84286\n",
      "Epoch 345/4000\n",
      " - 0s - loss: 0.1334 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00345: val_accuracy did not improve from 0.84286\n",
      "Epoch 346/4000\n",
      " - 0s - loss: 0.1334 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00346: val_accuracy did not improve from 0.84286\n",
      "Epoch 347/4000\n",
      " - 0s - loss: 0.1333 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00347: val_accuracy did not improve from 0.84286\n",
      "Epoch 348/4000\n",
      " - 0s - loss: 0.1332 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00348: val_accuracy did not improve from 0.84286\n",
      "Epoch 349/4000\n",
      " - 0s - loss: 0.1332 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00349: val_accuracy did not improve from 0.84286\n",
      "Epoch 350/4000\n",
      " - 0s - loss: 0.1331 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00350: val_accuracy did not improve from 0.84286\n",
      "Epoch 351/4000\n",
      " - 0s - loss: 0.1330 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00351: val_accuracy did not improve from 0.84286\n",
      "Epoch 352/4000\n",
      " - 0s - loss: 0.1330 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00352: val_accuracy did not improve from 0.84286\n",
      "Epoch 353/4000\n",
      " - 0s - loss: 0.1329 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00353: val_accuracy did not improve from 0.84286\n",
      "Epoch 354/4000\n",
      " - 0s - loss: 0.1328 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00354: val_accuracy did not improve from 0.84286\n",
      "Epoch 355/4000\n",
      " - 0s - loss: 0.1328 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00355: val_accuracy did not improve from 0.84286\n",
      "Epoch 356/4000\n",
      " - 0s - loss: 0.1327 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00356: val_accuracy did not improve from 0.84286\n",
      "Epoch 357/4000\n",
      " - 0s - loss: 0.1326 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00357: val_accuracy did not improve from 0.84286\n",
      "Epoch 358/4000\n",
      " - 0s - loss: 0.1326 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00358: val_accuracy did not improve from 0.84286\n",
      "Epoch 359/4000\n",
      " - 0s - loss: 0.1325 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00359: val_accuracy did not improve from 0.84286\n",
      "Epoch 360/4000\n",
      " - 0s - loss: 0.1324 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00360: val_accuracy did not improve from 0.84286\n",
      "Epoch 361/4000\n",
      " - 0s - loss: 0.1323 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00361: val_accuracy did not improve from 0.84286\n",
      "Epoch 362/4000\n",
      " - 0s - loss: 0.1323 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00362: val_accuracy did not improve from 0.84286\n",
      "Epoch 363/4000\n",
      " - 0s - loss: 0.1322 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00363: val_accuracy did not improve from 0.84286\n",
      "Epoch 364/4000\n",
      " - 0s - loss: 0.1321 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00364: val_accuracy did not improve from 0.84286\n",
      "Epoch 365/4000\n",
      " - 0s - loss: 0.1321 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00365: val_accuracy did not improve from 0.84286\n",
      "Epoch 366/4000\n",
      " - 0s - loss: 0.1320 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00366: val_accuracy did not improve from 0.84286\n",
      "Epoch 367/4000\n",
      " - 0s - loss: 0.1319 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00367: val_accuracy did not improve from 0.84286\n",
      "Epoch 368/4000\n",
      " - 0s - loss: 0.1318 - accuracy: 0.9667 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00368: val_accuracy did not improve from 0.84286\n",
      "Epoch 369/4000\n",
      " - 0s - loss: 0.1318 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00369: val_accuracy did not improve from 0.84286\n",
      "Epoch 370/4000\n",
      " - 0s - loss: 0.1317 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00370: val_accuracy did not improve from 0.84286\n",
      "Epoch 371/4000\n",
      " - 0s - loss: 0.1316 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00371: val_accuracy did not improve from 0.84286\n",
      "Epoch 372/4000\n",
      " - 0s - loss: 0.1315 - accuracy: 0.9667 - val_loss: 0.3767 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00372: val_accuracy did not improve from 0.84286\n",
      "Epoch 373/4000\n",
      " - 0s - loss: 0.1314 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00373: val_accuracy did not improve from 0.84286\n",
      "Epoch 374/4000\n",
      " - 0s - loss: 0.1314 - accuracy: 0.9667 - val_loss: 0.3766 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00374: val_accuracy did not improve from 0.84286\n",
      "Epoch 375/4000\n",
      " - 0s - loss: 0.1313 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00375: val_accuracy did not improve from 0.84286\n",
      "Epoch 376/4000\n",
      " - 0s - loss: 0.1312 - accuracy: 0.9667 - val_loss: 0.3765 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00376: val_accuracy did not improve from 0.84286\n",
      "Epoch 377/4000\n",
      " - 0s - loss: 0.1311 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00377: val_accuracy did not improve from 0.84286\n",
      "Epoch 378/4000\n",
      " - 0s - loss: 0.1311 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00378: val_accuracy did not improve from 0.84286\n",
      "Epoch 379/4000\n",
      " - 0s - loss: 0.1310 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00379: val_accuracy did not improve from 0.84286\n",
      "Epoch 380/4000\n",
      " - 0s - loss: 0.1309 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00380: val_accuracy did not improve from 0.84286\n",
      "Epoch 381/4000\n",
      " - 0s - loss: 0.1308 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00381: val_accuracy did not improve from 0.84286\n",
      "Epoch 382/4000\n",
      " - 0s - loss: 0.1307 - accuracy: 0.9667 - val_loss: 0.3764 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00382: val_accuracy did not improve from 0.84286\n",
      "Epoch 383/4000\n",
      " - 0s - loss: 0.1307 - accuracy: 0.9667 - val_loss: 0.3763 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00383: val_accuracy did not improve from 0.84286\n",
      "Epoch 384/4000\n",
      " - 0s - loss: 0.1306 - accuracy: 0.9667 - val_loss: 0.3763 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00384: val_accuracy did not improve from 0.84286\n",
      "Epoch 385/4000\n",
      " - 0s - loss: 0.1305 - accuracy: 0.9667 - val_loss: 0.3762 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00385: val_accuracy did not improve from 0.84286\n",
      "Epoch 386/4000\n",
      " - 0s - loss: 0.1304 - accuracy: 0.9667 - val_loss: 0.3762 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00386: val_accuracy did not improve from 0.84286\n",
      "Epoch 387/4000\n",
      " - 0s - loss: 0.1303 - accuracy: 0.9667 - val_loss: 0.3761 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00387: val_accuracy did not improve from 0.84286\n",
      "Epoch 388/4000\n",
      " - 0s - loss: 0.1302 - accuracy: 0.9667 - val_loss: 0.3761 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00388: val_accuracy did not improve from 0.84286\n",
      "Epoch 389/4000\n",
      " - 0s - loss: 0.1302 - accuracy: 0.9667 - val_loss: 0.3760 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00389: val_accuracy did not improve from 0.84286\n",
      "Epoch 390/4000\n",
      " - 0s - loss: 0.1301 - accuracy: 0.9667 - val_loss: 0.3759 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00390: val_accuracy did not improve from 0.84286\n",
      "Epoch 391/4000\n",
      " - 0s - loss: 0.1300 - accuracy: 0.9667 - val_loss: 0.3758 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00391: val_accuracy did not improve from 0.84286\n",
      "Epoch 392/4000\n",
      " - 0s - loss: 0.1299 - accuracy: 0.9667 - val_loss: 0.3758 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00392: val_accuracy did not improve from 0.84286\n",
      "Epoch 393/4000\n",
      " - 0s - loss: 0.1298 - accuracy: 0.9667 - val_loss: 0.3757 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00393: val_accuracy did not improve from 0.84286\n",
      "Epoch 394/4000\n",
      " - 0s - loss: 0.1297 - accuracy: 0.9667 - val_loss: 0.3757 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00394: val_accuracy did not improve from 0.84286\n",
      "Epoch 395/4000\n",
      " - 0s - loss: 0.1296 - accuracy: 0.9667 - val_loss: 0.3757 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00395: val_accuracy did not improve from 0.84286\n",
      "Epoch 396/4000\n",
      " - 0s - loss: 0.1296 - accuracy: 0.9667 - val_loss: 0.3757 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00396: val_accuracy did not improve from 0.84286\n",
      "Epoch 397/4000\n",
      " - 0s - loss: 0.1295 - accuracy: 0.9667 - val_loss: 0.3756 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00397: val_accuracy did not improve from 0.84286\n",
      "Epoch 398/4000\n",
      " - 0s - loss: 0.1294 - accuracy: 0.9667 - val_loss: 0.3755 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00398: val_accuracy did not improve from 0.84286\n",
      "Epoch 399/4000\n",
      " - 0s - loss: 0.1293 - accuracy: 0.9667 - val_loss: 0.3754 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00399: val_accuracy did not improve from 0.84286\n",
      "Epoch 400/4000\n",
      " - 0s - loss: 0.1292 - accuracy: 0.9667 - val_loss: 0.3754 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00400: val_accuracy did not improve from 0.84286\n",
      "Epoch 401/4000\n",
      " - 0s - loss: 0.1291 - accuracy: 0.9667 - val_loss: 0.3753 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00401: val_accuracy did not improve from 0.84286\n",
      "Epoch 402/4000\n",
      " - 0s - loss: 0.1290 - accuracy: 0.9667 - val_loss: 0.3752 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00402: val_accuracy did not improve from 0.84286\n",
      "Epoch 403/4000\n",
      " - 0s - loss: 0.1289 - accuracy: 0.9667 - val_loss: 0.3751 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00403: val_accuracy did not improve from 0.84286\n",
      "Epoch 404/4000\n",
      " - 0s - loss: 0.1288 - accuracy: 0.9667 - val_loss: 0.3750 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00404: val_accuracy did not improve from 0.84286\n",
      "Epoch 405/4000\n",
      " - 0s - loss: 0.1287 - accuracy: 0.9667 - val_loss: 0.3750 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00405: val_accuracy did not improve from 0.84286\n",
      "Epoch 406/4000\n",
      " - 0s - loss: 0.1286 - accuracy: 0.9667 - val_loss: 0.3749 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00406: val_accuracy did not improve from 0.84286\n",
      "Epoch 407/4000\n",
      " - 0s - loss: 0.1285 - accuracy: 0.9667 - val_loss: 0.3748 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00407: val_accuracy did not improve from 0.84286\n",
      "Epoch 408/4000\n",
      " - 0s - loss: 0.1284 - accuracy: 0.9667 - val_loss: 0.3747 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00408: val_accuracy did not improve from 0.84286\n",
      "Epoch 409/4000\n",
      " - 0s - loss: 0.1284 - accuracy: 0.9667 - val_loss: 0.3746 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00409: val_accuracy did not improve from 0.84286\n",
      "Epoch 410/4000\n",
      " - 0s - loss: 0.1283 - accuracy: 0.9667 - val_loss: 0.3745 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00410: val_accuracy did not improve from 0.84286\n",
      "Epoch 411/4000\n",
      " - 0s - loss: 0.1282 - accuracy: 0.9667 - val_loss: 0.3744 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00411: val_accuracy did not improve from 0.84286\n",
      "Epoch 412/4000\n",
      " - 0s - loss: 0.1281 - accuracy: 0.9667 - val_loss: 0.3743 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00412: val_accuracy did not improve from 0.84286\n",
      "Epoch 413/4000\n",
      " - 0s - loss: 0.1280 - accuracy: 0.9667 - val_loss: 0.3742 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00413: val_accuracy did not improve from 0.84286\n",
      "Epoch 414/4000\n",
      " - 0s - loss: 0.1279 - accuracy: 0.9667 - val_loss: 0.3742 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00414: val_accuracy did not improve from 0.84286\n",
      "Epoch 415/4000\n",
      " - 0s - loss: 0.1278 - accuracy: 0.9667 - val_loss: 0.3740 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00415: val_accuracy did not improve from 0.84286\n",
      "Epoch 416/4000\n",
      " - 0s - loss: 0.1277 - accuracy: 0.9667 - val_loss: 0.3739 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00416: val_accuracy did not improve from 0.84286\n",
      "Epoch 417/4000\n",
      " - 0s - loss: 0.1276 - accuracy: 0.9667 - val_loss: 0.3738 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00417: val_accuracy did not improve from 0.84286\n",
      "Epoch 00417: early stopping\n",
      "Train: 0.967, Test: 0.843\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "n_train = 30\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=2, callbacks=[es, mc])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
